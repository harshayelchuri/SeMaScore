# -*- coding: utf-8 -*-
"""Sema-score generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11MZ3bZvKhYEByrbecl-f3th2dy6bwVSL
"""

!pip3 install torch
!pip3 install transformers
!pip3 install bert-score

import os
import sys
from collections import Counter, defaultdict
from functools import partial
from itertools import chain
from math import log
from multiprocessing import Pool
from packaging import version


import torch
from torch.nn.utils.rnn import pad_sequence
from tqdm.auto import tqdm
from transformers import (AutoModel, AutoTokenizer, BertConfig, GPT2Tokenizer, RobertaTokenizer,
                          RobertaConfig, XLMConfig, XLNetConfig)

from transformers import __version__ as trans_version
import pandas as  pd
import re
import traceback
from transformers import logging
logging.set_verbosity_error()

def get_model(model_type, num_layers, all_layers=None):
    model = AutoModel.from_pretrained(model_type)
    model.eval()

    if hasattr(model, "decoder") and hasattr(model, "encoder"):
        model = model.encoder

    # drop unused layers
    if not all_layers:
        if hasattr(model, "n_layers"):  # xlm
            assert (
                0 <= num_layers <= model.n_layers
            ), f"Invalid num_layers: num_layers should be between 0 and {model.n_layers} for {model_type}"
            model.n_layers = num_layers
        elif hasattr(model, "layer"):  # xlnet
            assert (
                0 <= num_layers <= len(model.layer)
            ), f"Invalid num_layers: num_layers should be between 0 and {len(model.layer)} for {model_type}"
            model.layer = torch.nn.ModuleList(
                [layer for layer in model.layer[:num_layers]]
            )
        elif hasattr(model, "encoder"):  # albert
            if hasattr(model.encoder, "albert_layer_groups"):
                assert (
                    0 <= num_layers <= model.encoder.config.num_hidden_layers
                ), f"Invalid num_layers: num_layers should be between 0 and {model.encoder.config.num_hidden_layers} for {model_type}"
                model.encoder.config.num_hidden_layers = num_layers
            elif hasattr(model.encoder, "block"):  # t5
                assert (
                    0 <= num_layers <= len(model.encoder.block)
                ), f"Invalid num_layers: num_layers should be between 0 and {len(model.encoder.block)} for {model_type}"
                model.encoder.block = torch.nn.ModuleList(
                    [layer for layer in model.encoder.block[:num_layers]]
                )
            else:  # bert, roberta
                assert (
                    0 <= num_layers <= len(model.encoder.layer)
                ), f"Invalid num_layers: num_layers should be between 0 and {len(model.encoder.layer)} for {model_type}"
                model.encoder.layer = torch.nn.ModuleList(
                    [layer for layer in model.encoder.layer[:num_layers]]
                )
        elif hasattr(model, "transformer"):  # bert, roberta
            assert (
                0 <= num_layers <= len(model.transformer.layer)
            ), f"Invalid num_layers: num_layers should be between 0 and {len(model.transformer.layer)} for {model_type}"
            model.transformer.layer = torch.nn.ModuleList(
                [layer for layer in model.transformer.layer[:num_layers]]
            )
        elif hasattr(model, "layers"):  # bart
            assert (
                0 <= num_layers <= len(model.layers)
            ), f"Invalid num_layers: num_layers should be between 0 and {len(model.layers)} for {model_type}"
            model.layers = torch.nn.ModuleList(
                [layer for layer in model.layers[:num_layers]]
            )
        else:
            raise ValueError("Not supported")
    else:
        if hasattr(model, "output_hidden_states"):
            model.output_hidden_states = True
        elif hasattr(model, "encoder"):
            model.encoder.output_hidden_states = True
        elif hasattr(model, "transformer"):
            model.transformer.output_hidden_states = True
        # else:
        #     raise ValueError(f"Not supported model architecture: {model_type}")

    return model

def get_tokenizer(model_type, use_fast=False):
  tokenizer = AutoTokenizer.from_pretrained(model_type, use_fast=use_fast)
  return tokenizer

model_type = "microsoft/deberta-large-mnli"
    num_layers = 18
    tokenizer = get_tokenizer(model_type, use_fast=False)
    model = get_model(model_type, num_layers)

def padding(arr, pad_token, dtype=torch.long):
    lens = torch.LongTensor([len(a) for a in arr])
    max_len = lens.max().item()
    padded = torch.ones(len(arr), max_len, dtype=dtype) * pad_token
    mask = torch.zeros(len(arr), max_len, dtype=torch.long)
    for i, a in enumerate(arr):
        padded[i, : lens[i]] = torch.tensor(a, dtype=dtype)
        mask[i, : lens[i]] = 1
    return padded, lens, mask

def collate_idf(arr, tokenizer, idf_dict, device="cuda:0"):
    """
    Helper function that pads a list of sentences to hvae the same length and
    loads idf score for words in the sentences.

    Args:
        - :param: `arr` (list of str): sentences to process.
        - :param: `tokenize` : a function that takes a string and return list
                  of tokens.
        - :param: `numericalize` : a function that takes a list of tokens and
                  return list of token indexes.
        - :param: `idf_dict` (dict): mapping a word piece index to its
                               inverse document frequency
        - :param: `pad` (str): the padding token.
        - :param: `device` (str): device to use, e.g. 'cpu' or 'cuda'
    """
    arr = [sent_encode(tokenizer, a) for a in arr]

    idf_weights = [[idf_dict[i] for i in a] for a in arr]

    pad_token = tokenizer.pad_token_id

    padded, lens, mask = padding(arr, pad_token, dtype=torch.long)
    padded_idf, _, _ = padding(idf_weights, 0, dtype=torch.float)

    padded = padded.to(device=device)
    mask = mask.to(device=device)
    lens = lens.to(device=device)
    return padded, padded_idf, lens, mask


def bert_encode(model, x, attention_mask, all_layers=False):
    model.eval()
    with torch.no_grad():
        out = model(x, attention_mask=attention_mask, output_hidden_states=all_layers)
    if all_layers:
        emb = torch.stack(out[-1], dim=2)
    else:
        emb = out[0]
    return emb

def get_bert_embedding(
    all_sens,
    model,
    tokenizer,
    idf_dict,
    batch_size=-1,
    device="cuda:0",
    all_layers=False,
):
    """
    Compute BERT embedding in batches.

    Args:
        - :param: `all_sens` (list of str) : sentences to encode.
        - :param: `model` : a BERT model from `pytorch_pretrained_bert`.
        - :param: `tokenizer` : a BERT tokenizer corresponds to `model`.
        - :param: `idf_dict` (dict) : mapping a word piece index to its
                               inverse document frequency
        - :param: `device` (str): device to use, e.g. 'cpu' or 'cuda'
    """

    padded_sens, padded_idf, lens, mask = collate_idf(
        all_sens, tokenizer, idf_dict, device=device
    )

    if batch_size == -1:
        batch_size = len(all_sens)

    embeddings = []
    with torch.no_grad():
        for i in range(0, len(all_sens), batch_size):
            batch_embedding = bert_encode(
                model,
                padded_sens[i : i + batch_size],
                attention_mask=mask[i : i + batch_size],
                all_layers=all_layers,
            )
            embeddings.append(batch_embedding)
            del batch_embedding

    total_embedding = torch.cat(embeddings, dim=0)

    return total_embedding, mask, padded_idf

def sent_encode(tokenizer, sent):
    "Encoding as sentence based on the tokenizer"
    sent = sent.strip()
    if sent == "":
        return tokenizer.build_inputs_with_special_tokens([])
    elif isinstance(tokenizer, GPT2Tokenizer) or isinstance(tokenizer, RobertaTokenizer):
        # for RoBERTa and GPT-2
        if version.parse(trans_version) >= version.parse("4.0.0"):
            return tokenizer.encode(
                sent,
                add_special_tokens=True,
                add_prefix_space=True,
                max_length=tokenizer.model_max_length,
                truncation=True,
            )
        elif version.parse(trans_version) >= version.parse("3.0.0"):
            return tokenizer.encode(
                sent,
                add_special_tokens=True,
                add_prefix_space=True,
                max_length=tokenizer.max_len,
                truncation=True,
            )
        elif version.parse(trans_version) >= version.parse("2.0.0"):
            return tokenizer.encode(
                sent,
                add_special_tokens=True,
                add_prefix_space=True,
                max_length=tokenizer.max_len,
            )
        else:
            raise NotImplementedError(
                f"transformers version {trans_version} is not supported"
            )
    else:
        if version.parse(trans_version) >= version.parse("4.0.0"):
            return tokenizer.encode(
                sent,
                add_special_tokens=True,
                max_length=tokenizer.model_max_length,
                truncation=True,
            )
        elif version.parse(trans_version) >= version.parse("3.0.0"):
            return tokenizer.encode(
                sent,
                add_special_tokens=True,
                max_length=tokenizer.max_len,
                truncation=True,
            )
        elif version.parse(trans_version) >= version.parse("2.0.0"):
            return tokenizer.encode(
                sent, add_special_tokens=True, max_length=tokenizer.max_len
            )
        else:
            raise NotImplementedError(
                f"transformers version {trans_version} is not supported"
            )


idf_dict = defaultdict(lambda: 1.0)
# set idf for [SEP] and [CLS] to 0
idf_dict[tokenizer.sep_token_id] = 0
idf_dict[tokenizer.cls_token_id] = 0

candidate = "smoke kills"
reference = "smoking kills"
device = 'cpu'

hyp_embedding, masks, padded_idf = get_bert_embedding([candidate], model, tokenizer, idf_dict, device=device, all_layers=False)
ref_embedding, masks, padded_idf = get_bert_embedding([reference], model, tokenizer, idf_dict, device=device, all_layers=False)

# Function to get the transformed ground truth after applying edit-distance
def printChanges(s1, s2, dp):
    x=[]
    i = len(s1)
    j = len(s2)

   # Check till the end
    while(i > 0 and j > 0):

        # If characters are same
        if s1[i - 1] == s2[j - 1]:
            x.append(s1[i - 1])
            i -= 1
            j -= 1


        # Replace
        elif dp[i][j] == dp[i - 1][j - 1] + 1:
            # print("change", s1[i - 1],
                    #   "to", s2[j - 1])
            j -= 1
            i -= 1
            x.append('$')

        # Delete
        elif dp[i][j] == dp[i - 1][j] + 1:
            # print("Delete", s1[i - 1])
            i -= 1
            x.append('-')

        # Add
        elif dp[i][j] == dp[i][j - 1] + 1:
            # print("Add", s2[j - 1])
            j -= 1
            x.append('+')
    while i>0:
        x.append('-')
        # print("Add", s2[i - 1])
        i-=1
    while j>0:
        x.append('+')
        # print("Add", s2[j - 1])
        j-=1
    return x

# Funtion to compute edit-distance
def editDP(s1, s2):

    len1 = len(s1)
    len2 = len(s2)
    dp = [[0 for i in range(len2 + 1)]
             for j in range(len1 + 1)]

    # Initialize by the maximum edits possible
    for i in range(len1 + 1):
        dp[i][0] = i
    for j in range(len2 + 1):
        dp[0][j] = j

    # Compute the DP Matrix
    for i in range(1, len1 + 1):
        for j in range(1, len2 + 1):

            # If the characters are same
            # no changes required
            if s2[j - 1] == s1[i - 1]:
                dp[i][j] = dp[i - 1][j - 1]

            # Minimum of three operations possible
            else:
                dp[i][j] = 1 + min(dp[i][j - 1],
                                   dp[i - 1][j - 1],
                                   dp[i - 1][j])

    # Print the steps
    x=printChanges(s1, s2, dp)
    return x

# FUnction to call the edit-distance function
def hit_values(ref_li,hyp_li):
    x=[]
    x=editDP(hyp_li,ref_li)
    print(ref_li)
    print(hyp_li)
    aligned=x[::-1]
    aligned=''.join(map(str, aligned))
    print(aligned)
    return aligned

# Funtions to map the ground truth and hypothesis with the transformed ground truth
def sub_sentence_mapper1(ground_truth,aligned):
    split_ground_truth_v1=[]
    split_aligned_v1=[]
    count=0
    index=0
    temp_str1,temp_str2='',''
    while count<len(ground_truth) and index<len(aligned):
        if (ground_truth[count]==' ' and aligned[index]==' '):# or (ground_truth[count]==' ' and aligned[index]=='+'):
            split_ground_truth_v1.append(temp_str1)
            split_aligned_v1.append(temp_str2)
            temp_str1=''
            temp_str2=''
            count+=1
            index+=1
        else:
            if aligned[index]=='-':
                temp_str2+=aligned[index]
                index+=1
            else:
                temp_str1+=ground_truth[count]
                temp_str2+=aligned[index]
                count+=1
                index+=1

    while count<len(ground_truth):
        temp_str1+=ground_truth[count]
        count+=1

    while index<len(aligned):
        temp_str2+=aligned[index]
        index+=1
    split_ground_truth_v1.append(temp_str1)
    split_aligned_v1.append(temp_str2)
    print(f'#{split_ground_truth_v1}')
    print(f'#{split_aligned_v1}')
    return split_ground_truth_v1, len(split_ground_truth_v1)==len(split_aligned_v1)

def sub_sentence_mapper2(ground_truth,aligned):
    split_ground_truth_v1=[]
    split_aligned_v1=[]
    count=0
    index=0
    temp_str1,temp_str2='',''
    while count<len(ground_truth) and index<len(aligned):
        if (ground_truth[count]==' ' and aligned[index]==' '):# or (ground_truth[count]==' ' and aligned[index]=='+'):
            split_ground_truth_v1.append(temp_str1)
            split_aligned_v1.append(temp_str2)
            temp_str1=''
            temp_str2=''
            count+=1
            index+=1
        else:
            if aligned[index]=='+':
                temp_str2+=aligned[index]
                index+=1
            else:
                temp_str1+=ground_truth[count]
                temp_str2+=aligned[index]
                count+=1
                index+=1

    while count<len(ground_truth):
        temp_str1+=ground_truth[count]
        count+=1

    while index<len(aligned):
        temp_str2+=aligned[index]
        index+=1
    split_ground_truth_v1.append(temp_str1)
    split_aligned_v1.append(temp_str2)
    print(f'#{split_ground_truth_v1}')
    print(f'#{split_aligned_v1}')
    print(len(split_ground_truth_v1)==len(split_aligned_v1))
    return split_ground_truth_v1, len(split_ground_truth_v1)==len(split_aligned_v1)

# Funtion to compute (1 - match error rate)
def get_mer(ground_truth,inference):
    aligned=hit_values(ground_truth,inference)
    mismatches=0
    for i in aligned:
        if i in ('+','-','$'):
            mismatches+=1
    mer=mismatches/max(len(ground_truth),len(inference))
    return 1-mer

# Function to get the aligned ground truth and aligned hypothesis along with (1 - match error rate)
def mapped_sentence(ground_truth,inference):
    aligned=hit_values(ground_truth,inference)
    mismatches=0
    for i in aligned:
        if i in ('+','-','$'):
            mismatches+=1
    mer=mismatches/max(len(ground_truth),len(inference))
    # split_ground_truth=ground_truth.split(' ')
    split_ground_truth_v1=[]
    split_inference=inference.split(' ')
    split_aligned_v1=[]
    count=0
    index=0
    temp_str1,temp_str2='',''

    mapped_ground_truth,mapped_ground_truth_res=sub_sentence_mapper1(ground_truth,aligned)
    mapped_inference,mapped_inference_res=sub_sentence_mapper2(inference,aligned)
    if mapped_ground_truth_res and mapped_inference_res:
        return mapped_ground_truth,mapped_inference,aligned,mer
    else:
        return False, False, False, False

# Function to compute cosine similarity
def cos_sim(a,b):
  a=a.unsqueeze(0)
  b=b.unsqueeze(0)
  a_norm = torch.nn.functional.normalize(a, p=2, dim=1)
  b_norm = torch.nn.functional.normalize(b, p=2, dim=1)
  # print(a_norm.shape)
  # print(b_norm.shape)
  ss  =  torch.mm(a_norm, b_norm.transpose(0, 1)).item()
  print(f'SS:{ss}')
  return ss

# Function to get the embeeddings for each segment of aligned ground truth
def get_gt_embeddings_v1(ground_truth_v1,gt_embedding,gt_tokens):
  start,end,k=0,0,0
  gt_embeddings_v1=[]
  word=''
  test_gt=[]
  for j in range(len(ground_truth_v1)):
    # print(k)
    word=''
    start=k+1
    while k<len(gt_tokens):
      word+=gt_tokens[k]
      print(f'word:{word},ground_truth_v1[j]:{ground_truth_v1[j]}')
      if word.strip()==ground_truth_v1[j].strip():
        test_gt.append(word)
        gt_embeddings_v1.append(torch.mean(gt_embedding[0][start:k+2],dim=0))
        print((start,k+2))
        k+=1
        break
      k+=1
  print(test_gt)
  print(len(gt_embeddings_v1),len(ground_truth_v1))
  return gt_embeddings_v1

# Function to get the embeddings for each segment of aligned hypothesis
def get_hyp_embeddings_v1(aligned_v1,hyp_embedding,hyp_tokens):
  start,end,k=0,0,0
  hyp_embeddings_v1=[]
  word=''
  test_hyp=[]
  for j in range(len(aligned_v1)):
    # print(k)
    word=''
    start=k+1
    while k<len(hyp_tokens):
      word+=hyp_tokens[k]
      print(f'word:{word},aligned_v1[j]:{aligned_v1[j]}')
      if word.strip()==aligned_v1[j].strip():
        test_hyp.append(word)
        hyp_embeddings_v1.append(torch.mean(hyp_embedding[0][start:k+2],dim=0))
        print((start,k+2))
        k+=1
        break
      k+=1
  print(test_hyp)
  print(len(hyp_embeddings_v1),len(aligned_v1))
  return hyp_embeddings_v1

# Function to get generate semascore
def generate_sema_score(ground_truth,hypothesis):
  ground_truth = re.sub(r'[^\w\s]', '', ground_truth.lower())
  hypothesis = re.sub(r'[^\w\s]', '', hypothesis.lower())
  gt_embedding, masks, padded_idf = get_bert_embedding([ground_truth], model, tokenizer, idf_dict, device=device, all_layers=False)
  hyp_embedding, masks, padded_idf = get_bert_embedding([hypothesis], model, tokenizer, idf_dict, device=device, all_layers=False)
  ground_truth_v1,aligned_v1,aligned,mer=mapped_sentence(ground_truth,hypothesis)
  gt_tokens = [tokenizer.decode([i]) for i in sent_encode(tokenizer, ground_truth)][1:-1]
  hyp_tokens = [tokenizer.decode([i]) for i in sent_encode(tokenizer, hypothesis)][1:-1]
  print(ground_truth_v1)
  print(gt_tokens)
  print(aligned_v1)
  print(hyp_tokens)

  gt_embeddings_v1=get_gt_embeddings_v1(ground_truth_v1,gt_embedding,gt_tokens)
  hyp_embeddings_v1=get_hyp_embeddings_v1(aligned_v1,hyp_embedding,hyp_tokens)
  total_gt_embedding=torch.mean(gt_embedding[0][1:-1],dim=0)
  ss_list=[]
  importance_list=[]
  multiplication_list=[]
  mer_list=[]
  average=0
  metric=0
  for j in range(len(gt_embeddings_v1)):
      mer_word=0
      importance = cos_sim(gt_embeddings_v1[j], total_gt_embedding)
      ss = cos_sim(gt_embeddings_v1[j], hyp_embeddings_v1[j])
      importance=(importance+1)/2
      ss=(ss+1)/2
      mer_word=get_mer(ground_truth_v1[j],aligned_v1[j])
      mer_list.append(mer_word)
      metric+=importance*ss*mer_word
      average+=importance
      ss_list.append(round(ss,4))
      importance_list.append(round(importance,4))
      multiplication_list.append(round(importance*ss,4))
      print(f'{round(ss,4)}#{ground_truth_v1[j]}#{aligned_v1[j]}')
      print(f'{round(importance,4)}#{ground_truth_v1[j]}#{ground_truth}')
  metric/=average
  print(metric)
  return (metric,ground_truth_v1,aligned_v1,aligned,ss_list,importance_list,multiplication_list,mer_list)

# Function to generate BERTScore
from bert_score import score
def generate_bert_score_v1(ground_truth,hypothesis):
    (P, R, F)= score([ground_truth], [hypothesis], lang="en",model_type='roberta-base',idf=False)
    print(f"P={P.mean().item():.6f} R={R.mean().item():.6f} F={F.mean().item():.6f}")
    bert_score_v1 = F.mean().item()
    print('Done with Bert_score_v1')
    return bert_score_v1

# Function to generate BERTscore and SeMaScore
def generate_Scores(path):
    df=pd.read_csv(path) #voicebank
    # df=pd.read_excel(path) #torgo
    exception_data={}

    for i in range(len(df)):
        try:
          new_df = df.iloc[[i]].copy()
          for j in range(1):
            print('i:',i)
            exception_data={}
            ground_truth=df.loc[i,'original'].strip()
            ground_truth = re.sub(r'[^\w\s]', '', ground_truth)
            hypothesis=df.loc[i,'processed'+str(j)].strip()
            hypothesis = re.sub(r'[^\w\s]', '', hypothesis)

            print(ground_truth,hypothesis)

            bert_score_v1=generate_bert_score_v1(ground_truth,hypothesis)
            sema_score=generate_sema_score(ground_truth,hypothesis)


            new_df['bert_score_v_mapped'+str(j)]=bert_score_v1
            new_df['sema_score'+str(j)]=sema_score[0]
            new_df['sema_score_gt'+str(j)]=str(sema_score[1])
            new_df['sema_score_inf'+str(j)]=str(sema_score[2])
            new_df['aligned'+str(j)]=str(sema_score[3])
            new_df['ss_list'+str(j)]=str(sema_score[4])
            new_df['importance_list'+str(j)]=str(sema_score[5])
            new_df['multiplication_list'+str(j)]=str(sema_score[6])
            new_df['mea_list'+str(j)]=str(sema_score[7])
          saving_path=path[:path.rindex('/')]+'/DS2_outputs_ATIS_new_dataset_v2_with_results_with_wer_sema_score.csv'
          print(saving_path)
          print(ground_truth)
          new_df.to_csv(saving_path,mode='a',index=False,header=(not os.path.exists(saving_path)))

        except Exception as e:
            traceback.print_tb(e.__traceback__)
            print('try exception error')
            exception_data['ground_truth'] = [ground_truth]
            exception_df=pd.DataFrame(exception_data)
            saving_path=path[:path.rindex('/')]+'/DS2_outputs_ATIS_new_dataset_v2_with_results_with_wer_mapped_score_with_exceptions.csv'
            new_df.to_csv(saving_path,mode='a',index=False,header=(not os.path.exists(saving_path)))
            exception_df.to_csv(saving_path,mode='a',index=False,header=(not os.path.exists(saving_path)))

generate_Scores(r'/content/DS2_outputs_ATIS_new_dataset_v2_with_results_with_wer.csv')

